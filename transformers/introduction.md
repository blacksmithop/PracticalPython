# ðŸªœ Introduction

Transformers work on the concept of self-attention.

Given a sequence, a Transformer model can relate different portions of the sequence.

<figure><img src="../.gitbook/assets/image (2) (1).png" alt="" width="293"><figcaption><p>Concept</p></figcaption></figure>

<figure><img src="../.gitbook/assets/image (4).png" alt="" width="375"><figcaption><p>Representation</p></figcaption></figure>

Transformers are popular for their use in generative tasks. Here's a representation of relations being mapped between a word and words that come before it in a sentence.&#x20;

<figure><img src="../.gitbook/assets/image (5).png" alt=""><figcaption><p>Relation mapping</p></figcaption></figure>

#### Why use Transformers instead of an RNN?

<figure><img src="../.gitbook/assets/image (2).png" alt=""><figcaption><p>Transformer vs RNN</p></figcaption></figure>

<figure><img src="../.gitbook/assets/image (8).png" alt=""><figcaption><p>GRU, LSTM vs RNN</p></figcaption></figure>

## Transformer Architecture

<figure><img src="../.gitbook/assets/image (7).png" alt=""><figcaption><p>Architecture</p></figcaption></figure>

### 1. Input Embedding

<figure><img src="../.gitbook/assets/image.png" alt=""><figcaption><p>Input embedding</p></figcaption></figure>

### 2. Positional Encoding

<figure><img src="../.gitbook/assets/image (1).png" alt=""><figcaption><p>Positional Encoding</p></figcaption></figure>

### 3. Multi-headed attention

<figure><img src="../.gitbook/assets/image (6).png" alt=""><figcaption><p>Multi-headed attention</p></figcaption></figure>
