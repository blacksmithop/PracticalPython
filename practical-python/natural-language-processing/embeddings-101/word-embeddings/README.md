---
description: Word embedding concepts implemented
---

# Word Embeddings

### Word encoding methods

* [One-hot encoding](one-hot-encoding.md)
* [Numerical encoding](numerical-encoding.md)

### Introduction

Word embeddings in short are a dense, efficient representation where similar words have a similar encoding. In effect they are a _dense_ vector of floating point values.

Much like how you train a Machine Learning model to learn it's weights, the values for embedding are trainable parameters.&#x20;

![](../../../../.gitbook/assets/image.png)

A _higher_ dimensional embedding can capture precise relationships between words but in exchange take more data to learn.
